# üîπ Module 2, Lesson 2.1: Project Structure and Environment

## üìö Learning Objectives
By the end of this lesson, you will:
- Set up a complete, modular DINO project structure
- Install and configure all required packages and dependencies
- Configure CIFAR-10 dataset for initial experiments
- Create a scalable codebase architecture for DINO implementation
- Understand best practices for deep learning project organization

---

## üèóÔ∏è DINO Project Architecture Overview

### Project Structure Philosophy
Our DINO implementation follows these principles:
1. **Modularity**: Each component is separate and reusable
2. **Scalability**: Easy to extend to new datasets and architectures
3. **Reproducibility**: Clear configuration and logging
4. **Maintainability**: Clean code with proper documentation

### üìÅ Complete Project Structure

```
dino_implementation/
‚îú‚îÄ‚îÄ üìÅ configs/                    # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ base_config.yaml          # Base configuration
‚îÇ   ‚îú‚îÄ‚îÄ cifar10_config.yaml       # CIFAR-10 specific settings
‚îÇ   ‚îî‚îÄ‚îÄ imagenet_config.yaml      # ImageNet specific settings
‚îú‚îÄ‚îÄ üìÅ data/                       # Data handling
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ datasets.py                # Dataset classes
‚îÇ   ‚îú‚îÄ‚îÄ augmentations.py           # Multi-crop augmentations
‚îÇ   ‚îî‚îÄ‚îÄ dataloaders.py             # DataLoader utilities
‚îú‚îÄ‚îÄ üìÅ models/                     # Model architectures
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ backbones/                 # Backbone networks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resnet.py              # ResNet implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vit.py                 # ViT implementation
‚îÇ   ‚îú‚îÄ‚îÄ heads.py                   # Projection heads
‚îÇ   ‚îî‚îÄ‚îÄ dino_model.py              # Complete DINO model
‚îú‚îÄ‚îÄ üìÅ training/                   # Training components
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ loss.py                    # DINO loss function
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py                 # Training logic
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Training utilities
‚îú‚îÄ‚îÄ üìÅ evaluation/                 # Evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ knn_eval.py                # k-NN evaluation
‚îÇ   ‚îú‚îÄ‚îÄ linear_probe.py            # Linear probing
‚îÇ   ‚îî‚îÄ‚îÄ attention_viz.py           # Attention visualization
‚îú‚îÄ‚îÄ üìÅ utils/                      # General utilities
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuration handling
‚îÇ   ‚îú‚îÄ‚îÄ logging.py                 # Logging utilities
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint.py              # Checkpoint management
‚îú‚îÄ‚îÄ üìÅ scripts/                    # Training and evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ train_dino.py              # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ eval_knn.py                # k-NN evaluation script
‚îÇ   ‚îî‚îÄ‚îÄ visualize_attention.py     # Attention visualization
‚îú‚îÄ‚îÄ üìÅ notebooks/                  # Jupyter notebooks for exploration
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_model_testing.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_results_analysis.ipynb
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ setup.py                       # Package setup
‚îú‚îÄ‚îÄ README.md                      # Project documentation
‚îî‚îÄ‚îÄ .gitignore                     # Git ignore file
```

---

## üõ†Ô∏è Environment Setup

### Step 1: Create Project Directory

```powershell
# Create main project directory
New-Item -ItemType Directory -Path "dino_implementation" -Force
Set-Location "dino_implementation"

# Create all subdirectories
$directories = @(
    "configs", "data", "models", "models/backbones", "training", 
    "evaluation", "utils", "scripts", "notebooks", "logs", "checkpoints"
)

foreach ($dir in $directories) {
    New-Item -ItemType Directory -Path $dir -Force
}

# Create __init__.py files for Python packages
$pythonPackages = @("data", "models", "models/backbones", "training", "evaluation", "utils")
foreach ($pkg in $pythonPackages) {
    New-Item -ItemType File -Path "$pkg/__init__.py" -Force
}
```

### Step 2: Python Environment Setup

```powershell
# Create virtual environment
python -m venv dino_env

# Activate virtual environment
.\dino_env\Scripts\Activate.ps1

# Upgrade pip
python -m pip install --upgrade pip
```

### Step 3: Install Dependencies

Create `requirements.txt`:

```txt
# Core ML libraries
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Computer vision and image processing
opencv-python>=4.8.0
Pillow>=9.5.0
albumentations>=1.3.0

# Scientific computing
numpy>=1.24.0
scipy>=1.10.0
scikit-learn>=1.3.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0

# Experiment tracking and logging
wandb>=0.15.0
tensorboard>=2.13.0
tqdm>=4.65.0

# Configuration management
pyyaml>=6.0
omegaconf>=2.3.0
hydra-core>=1.3.0

# Jupyter and development
jupyter>=1.0.0
notebook>=6.5.0
ipywidgets>=8.0.0

# Code quality
black>=23.0.0
flake8>=6.0.0
pytest>=7.4.0

# Additional utilities
timm>=0.9.0  # PyTorch Image Models
einops>=0.6.0  # Tensor operations
```

Install all dependencies:

```powershell
pip install -r requirements.txt
```

---

## ‚öôÔ∏è Configuration System

### Base Configuration (`configs/base_config.yaml`)

```yaml
# Base DINO Configuration
experiment:
  name: "dino_base"
  seed: 42
  device: "cuda"  # or "cpu"
  
# Data configuration
data:
  dataset: "cifar10"  # cifar10, imagenet, custom
  data_path: "./data"
  batch_size: 256
  num_workers: 4
  pin_memory: true
  
  # Multi-crop settings
  global_crops_number: 2
  local_crops_number: 8
  global_crops_scale: [0.4, 1.0]
  local_crops_scale: [0.05, 0.4]
  global_crop_size: 224
  local_crop_size: 96

# Model configuration
model:
  backbone: "resnet50"  # resnet50, vit_small, vit_base
  projection_dim: 65536
  bottleneck_dim: 256
  hidden_dim: 2048
  use_bn_in_head: false
  norm_last_layer: true
  
# Training configuration
training:
  epochs: 100
  warmup_epochs: 10
  lr: 0.0005  # Base learning rate
  min_lr: 1e-6
  weight_decay: 0.04
  weight_decay_end: 0.4
  clip_grad: 3.0
  batch_size_per_gpu: 64
  
  # Teacher-student parameters
  momentum_teacher: 0.996
  teacher_temp: 0.04
  student_temp: 0.1
  warmup_teacher_temp: 0.04
  warmup_teacher_temp_epochs: 0
  
  # Centering parameters
  center_momentum: 0.9

# Logging and checkpointing
logging:
  log_dir: "./logs"
  checkpoint_dir: "./checkpoints"
  save_checkpoint_frequency: 10
  wandb_project: "dino-implementation"
  wandb_entity: null  # Set your wandb username

# Evaluation
evaluation:
  eval_frequency: 10
  knn_k: 20
  knn_temperature: 0.07
  nb_knn: [10, 20, 100, 200]
```

### CIFAR-10 Specific Configuration (`configs/cifar10_config.yaml`)

```yaml
# CIFAR-10 specific overrides
defaults:
  - base_config

experiment:
  name: "dino_cifar10"

data:
  dataset: "cifar10"
  global_crop_size: 32  # CIFAR-10 native size
  local_crop_size: 16
  global_crops_scale: [0.7, 1.0]  # Adjusted for small images
  local_crops_scale: [0.3, 0.7]

model:
  backbone: "resnet18"  # Smaller backbone for CIFAR-10
  projection_dim: 8192  # Smaller projection for faster training

training:
  epochs: 200
  batch_size_per_gpu: 128
  lr: 0.001
```

---

## üìä Dataset Configuration

### CIFAR-10 Dataset Setup (`data/datasets.py`)

```python
import os
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision
import torchvision.transforms as transforms
from PIL import Image
import numpy as np


class CIFAR10Dataset(Dataset):
    """
    CIFAR-10 dataset wrapper for DINO training
    
    Returns multiple augmented views of each image for self-supervised learning.
    """
    
    def __init__(self, root='./data', train=True, transform=None, download=True):
        self.dataset = torchvision.datasets.CIFAR10(
            root=root, train=train, download=download, transform=None
        )
        self.transform = transform
        
        # CIFAR-10 class names for reference
        self.classes = [
            'airplane', 'automobile', 'bird', 'cat', 'deer',
            'dog', 'frog', 'horse', 'ship', 'truck'
        ]
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        
        if self.transform is not None:
            # Apply multi-crop transformation
            crops = self.transform(image)
            return crops, label
        else:
            return image, label


class ImageNetDataset(Dataset):
    """
    ImageNet dataset wrapper for DINO training
    
    Assumes ImageNet is organized in standard format:
    root/
        train/
            class1/
                img1.jpg
                img2.jpg
                ...
            class2/
                ...
    """
    
    def __init__(self, root, split='train', transform=None):
        self.root = root
        self.split = split
        self.transform = transform
        
        # Build file list
        self.samples = []
        self.class_to_idx = {}
        
        split_dir = os.path.join(root, split)
        if os.path.exists(split_dir):
            classes = sorted(os.listdir(split_dir))
            for idx, class_name in enumerate(classes):
                self.class_to_idx[class_name] = idx
                class_dir = os.path.join(split_dir, class_name)
                
                if os.path.isdir(class_dir):
                    for img_name in os.listdir(class_dir):
                        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                            self.samples.append((
                                os.path.join(class_dir, img_name), idx
                            ))
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        # Load image
        image = Image.open(img_path).convert('RGB')
        
        if self.transform is not None:
            crops = self.transform(image)
            return crops, label
        else:
            return image, label


def get_dataset(config):
    """
    Factory function to create datasets based on configuration
    
    Args:
        config: Configuration object with dataset parameters
        
    Returns:
        train_dataset, val_dataset (if available)
    """
    dataset_name = config.data.dataset.lower()
    
    if dataset_name == 'cifar10':
        # Import multi-crop transform (will be implemented in next lesson)
        from data.augmentations import MultiCropAugmentation
        
        transform = MultiCropAugmentation(
            global_crops_number=config.data.global_crops_number,
            local_crops_number=config.data.local_crops_number,
            global_crops_scale=config.data.global_crops_scale,
            local_crops_scale=config.data.local_crops_scale,
            global_crop_size=config.data.global_crop_size,
            local_crop_size=config.data.local_crop_size,
        )
        
        train_dataset = CIFAR10Dataset(
            root=config.data.data_path,
            train=True,
            transform=transform,
            download=True
        )
        
        val_dataset = CIFAR10Dataset(
            root=config.data.data_path,
            train=False,
            transform=None,  # No augmentation for validation
            download=True
        )
        
        return train_dataset, val_dataset
        
    elif dataset_name == 'imagenet':
        from data.augmentations import MultiCropAugmentation
        
        transform = MultiCropAugmentation(
            global_crops_number=config.data.global_crops_number,
            local_crops_number=config.data.local_crops_number,
            global_crops_scale=config.data.global_crops_scale,
            local_crops_scale=config.data.local_crops_scale,
            global_crop_size=config.data.global_crop_size,
            local_crop_size=config.data.local_crop_size,
        )
        
        train_dataset = ImageNetDataset(
            root=config.data.data_path,
            split='train',
            transform=transform
        )
        
        val_dataset = ImageNetDataset(
            root=config.data.data_path,
            split='val',
            transform=None
        )
        
        return train_dataset, val_dataset
    
    else:
        raise ValueError(f"Unknown dataset: {dataset_name}")


def create_dataloader(dataset, config, shuffle=True):
    """
    Create DataLoader with appropriate settings
    
    Args:
        dataset: PyTorch dataset
        config: Configuration object
        shuffle: Whether to shuffle data
        
    Returns:
        DataLoader object
    """
    return DataLoader(
        dataset,
        batch_size=config.data.batch_size,
        shuffle=shuffle,
        num_workers=config.data.num_workers,
        pin_memory=config.data.pin_memory,
        drop_last=True,  # Important for consistent batch sizes
        persistent_workers=True if config.data.num_workers > 0 else False
    )
```

---

## üîß Configuration Management (`utils/config.py`)

```python
import os
import yaml
from omegaconf import OmegaConf, DictConfig
from typing import Any, Dict, Optional


class ConfigManager:
    """
    Configuration management for DINO experiments
    
    Handles loading, merging, and validation of configuration files.
    """
    
    def __init__(self, config_dir: str = "configs"):
        self.config_dir = config_dir
    
    def load_config(self, config_name: str, overrides: Optional[Dict[str, Any]] = None) -> DictConfig:
        """
        Load configuration from YAML file with optional overrides
        
        Args:
            config_name: Name of config file (without .yaml extension)
            overrides: Dictionary of configuration overrides
            
        Returns:
            OmegaConf configuration object
        """
        config_path = os.path.join(self.config_dir, f"{config_name}.yaml")
        
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Configuration file not found: {config_path}")
        
        # Load base configuration
        config = OmegaConf.load(config_path)
        
        # Handle defaults (if config extends another config)
        if 'defaults' in config:
            base_configs = config.defaults
            if not isinstance(base_configs, list):
                base_configs = [base_configs]
            
            merged_config = OmegaConf.create({})
            
            # Load and merge base configurations
            for base_config in base_configs:
                if isinstance(base_config, str):
                    base_path = os.path.join(self.config_dir, f"{base_config}.yaml")
                elif isinstance(base_config, dict) and len(base_config) == 1:
                    base_name = list(base_config.keys())[0]
                    base_path = os.path.join(self.config_dir, f"{base_name}.yaml")
                else:
                    continue
                
                if os.path.exists(base_path):
                    base_cfg = OmegaConf.load(base_path)
                    merged_config = OmegaConf.merge(merged_config, base_cfg)
            
            # Merge with current config (current config takes precedence)
            config = OmegaConf.merge(merged_config, config)
        
        # Apply overrides
        if overrides:
            override_config = OmegaConf.create(overrides)
            config = OmegaConf.merge(config, override_config)
        
        # Validate configuration
        self._validate_config(config)
        
        return config
    
    def _validate_config(self, config: DictConfig) -> None:
        """
        Validate configuration parameters
        
        Args:
            config: Configuration to validate
            
        Raises:
            ValueError: If configuration is invalid
        """
        # Check required sections
        required_sections = ['data', 'model', 'training']
        for section in required_sections:
            if section not in config:
                raise ValueError(f"Missing required configuration section: {section}")
        
        # Validate data configuration
        if config.data.batch_size <= 0:
            raise ValueError("Batch size must be positive")
        
        if config.data.num_workers < 0:
            raise ValueError("Number of workers must be non-negative")
        
        # Validate model configuration
        if config.model.projection_dim <= 0:
            raise ValueError("Projection dimension must be positive")
        
        # Validate training configuration
        if config.training.epochs <= 0:
            raise ValueError("Number of epochs must be positive")
        
        if config.training.lr <= 0:
            raise ValueError("Learning rate must be positive")
        
        if not (0 <= config.training.momentum_teacher < 1):
            raise ValueError("Teacher momentum must be in [0, 1)")
        
        if config.training.teacher_temp <= 0:
            raise ValueError("Teacher temperature must be positive")
        
        if config.training.student_temp <= 0:
            raise ValueError("Student temperature must be positive")
    
    def save_config(self, config: DictConfig, save_path: str) -> None:
        """
        Save configuration to file
        
        Args:
            config: Configuration to save
            save_path: Path to save configuration
        """
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, 'w') as f:
            OmegaConf.save(config, f)
    
    def print_config(self, config: DictConfig) -> None:
        """
        Pretty print configuration
        
        Args:
            config: Configuration to print
        """
        print("Configuration:")
        print("=" * 50)
        print(OmegaConf.to_yaml(config))
        print("=" * 50)


# Global config manager instance
config_manager = ConfigManager()


def load_config(config_name: str, overrides: Optional[Dict[str, Any]] = None) -> DictConfig:
    """
    Convenience function to load configuration
    
    Args:
        config_name: Name of configuration file
        overrides: Optional configuration overrides
        
    Returns:
        Loaded configuration
    """
    return config_manager.load_config(config_name, overrides)
```

---

## üìù Setup Script (`setup.py`)

```python
from setuptools import setup, find_packages

setup(
    name="dino-implementation",
    version="0.1.0",
    description="DINO (Self-Distillation with No Labels) Implementation",
    author="Your Name",
    author_email="your.email@example.com",
    packages=find_packages(),
    python_requires=">=3.8",
    install_requires=[
        "torch>=2.0.0",
        "torchvision>=0.15.0",
        "numpy>=1.24.0",
        "pyyaml>=6.0",
        "omegaconf>=2.3.0",
        "tqdm>=4.65.0",
        "matplotlib>=3.7.0",
        "opencv-python>=4.8.0",
        "Pillow>=9.5.0",
        "scikit-learn>=1.3.0",
        "wandb>=0.15.0",
        "timm>=0.9.0",
    ],
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
    ],
)
```

---

## üß™ **Hands-on Exercise**: Create Complete Project Structure

### Task
Set up the complete DINO implementation environment and verify everything works correctly.

### Step-by-Step Implementation

```powershell
# 1. Create project structure
Write-Host "Creating DINO project structure..." -ForegroundColor Green

# Create main directory
$projectPath = "dino_implementation"
if (Test-Path $projectPath) {
    Remove-Item $projectPath -Recurse -Force
}
New-Item -ItemType Directory -Path $projectPath -Force
Set-Location $projectPath

# Create directory structure
$directories = @(
    "configs", "data", "models", "models/backbones", 
    "training", "evaluation", "utils", "scripts", 
    "notebooks", "logs", "checkpoints"
)

foreach ($dir in $directories) {
    New-Item -ItemType Directory -Path $dir -Force
    Write-Host "Created directory: $dir" -ForegroundColor Yellow
}

# Create __init__.py files
$packages = @("data", "models", "models/backbones", "training", "evaluation", "utils")
foreach ($pkg in $packages) {
    New-Item -ItemType File -Path "$pkg/__init__.py" -Force
}

Write-Host "Project structure created successfully!" -ForegroundColor Green
```

### Verification Script (`scripts/verify_setup.py`)

```python
import os
import sys
import torch
import torchvision
import yaml
from pathlib import Path

def check_environment():
    """Verify that the environment is set up correctly"""
    
    print("üîç DINO Implementation Environment Verification")
    print("=" * 50)
    
    # Check Python version
    python_version = sys.version_info
    print(f"‚úì Python version: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    if python_version < (3, 8):
        print("‚ùå Python 3.8+ required")
        return False
    
    # Check PyTorch installation
    try:
        print(f"‚úì PyTorch version: {torch.__version__}")
        print(f"‚úì CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"‚úì CUDA version: {torch.version.cuda}")
            print(f"‚úì GPU count: {torch.cuda.device_count()}")
    except ImportError:
        print("‚ùå PyTorch not installed")
        return False
    
    # Check TorchVision
    try:
        print(f"‚úì TorchVision version: {torchvision.__version__}")
    except ImportError:
        print("‚ùå TorchVision not installed")
        return False
    
    # Check project structure
    expected_dirs = [
        "configs", "data", "models", "training", 
        "evaluation", "utils", "scripts", "notebooks"
    ]
    
    print("\nüìÅ Project Structure:")
    for dir_name in expected_dirs:
        if os.path.exists(dir_name):
            print(f"‚úì {dir_name}/")
        else:
            print(f"‚ùå {dir_name}/ missing")
            return False
    
    # Check required files
    required_files = [
        "requirements.txt",
        "configs/base_config.yaml",
        "configs/cifar10_config.yaml"
    ]
    
    print("\nüìÑ Required Files:")
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"‚úì {file_path}")
        else:
            print(f"‚ùå {file_path} missing")
    
    # Test configuration loading
    try:
        sys.path.append('.')
        from utils.config import load_config
        config = load_config('cifar10_config')
        print(f"‚úì Configuration loading works")
        print(f"‚úì Loaded config for dataset: {config.data.dataset}")
    except Exception as e:
        print(f"‚ùå Configuration loading failed: {e}")
        return False
    
    # Test dataset download
    try:
        print("\nüìä Testing CIFAR-10 download...")
        dataset = torchvision.datasets.CIFAR10(
            root='./data', 
            train=True, 
            download=True
        )
        print(f"‚úì CIFAR-10 downloaded successfully ({len(dataset)} samples)")
    except Exception as e:
        print(f"‚ùå CIFAR-10 download failed: {e}")
        return False
    
    print("\nüéâ Environment verification completed successfully!")
    print("You're ready to start implementing DINO!")
    
    return True

if __name__ == "__main__":
    success = check_environment()
    sys.exit(0 if success else 1)
```

### Run Verification

```powershell
# Run the verification script
python scripts/verify_setup.py
```

---

## üìã Project Documentation (`README.md`)

```markdown
# DINO Implementation

A complete PyTorch implementation of **DINO (Self-Distillation with No Labels)** for self-supervised learning.

## Overview

This project implements the DINO method from the paper ["Emerging Properties in Self-Supervised Vision Transformers"](https://arxiv.org/abs/2104.14294) by Caron et al.

## Features

- ‚úÖ Complete DINO implementation with multi-crop strategy
- ‚úÖ Support for ResNet and Vision Transformer backbones
- ‚úÖ Flexible configuration system
- ‚úÖ Comprehensive evaluation suite (k-NN, linear probing, attention visualization)
- ‚úÖ Integration with Weights & Biases for experiment tracking
- ‚úÖ Support for CIFAR-10 and ImageNet datasets

## Quick Start

### 1. Environment Setup

```bash
# Create virtual environment
python -m venv dino_env
source dino_env/bin/activate  # Linux/Mac
# or
.\dino_env\Scripts\Activate.ps1  # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Verify Installation

```bash
python scripts/verify_setup.py
```

### 3. Train DINO on CIFAR-10

```bash
python scripts/train_dino.py --config cifar10_config
```

## Project Structure

```
dino_implementation/
‚îú‚îÄ‚îÄ configs/           # Configuration files
‚îú‚îÄ‚îÄ data/             # Data handling and augmentations
‚îú‚îÄ‚îÄ models/           # Model architectures
‚îú‚îÄ‚îÄ training/         # Training logic and loss functions
‚îú‚îÄ‚îÄ evaluation/       # Evaluation scripts
‚îú‚îÄ‚îÄ utils/            # Utilities and helpers
‚îú‚îÄ‚îÄ scripts/          # Training and evaluation scripts
‚îî‚îÄ‚îÄ notebooks/        # Jupyter notebooks for analysis
```

## Configuration

The project uses YAML configuration files for easy experimentation:

- `configs/base_config.yaml`: Base configuration
- `configs/cifar10_config.yaml`: CIFAR-10 specific settings
- `configs/imagenet_config.yaml`: ImageNet specific settings

## Usage Examples

### Training

```bash
# Train on CIFAR-10
python scripts/train_dino.py --config cifar10_config

# Train with custom parameters
python scripts/train_dino.py --config cifar10_config \
    --overrides training.epochs=200 training.lr=0.001
```

### Evaluation

```bash
# k-NN evaluation
python scripts/eval_knn.py --checkpoint checkpoints/dino_cifar10_best.pth

# Linear probing
python evaluation/linear_probe.py --checkpoint checkpoints/dino_cifar10_best.pth
```

### Visualization

```bash
# Visualize attention maps
python scripts/visualize_attention.py --checkpoint checkpoints/dino_cifar10_best.pth
```

## Results

| Dataset | Backbone | k-NN Acc | Linear Acc | 
|---------|----------|----------|------------|
| CIFAR-10 | ResNet-18 | XX.X% | XX.X% |
| CIFAR-10 | ViT-Small | XX.X% | XX.X% |

## Citation

```bibtex
@inproceedings{caron2021emerging,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={International Conference on Computer Vision},
  year={2021}
}
```

## License

MIT License - see LICENSE file for details.
```

---

## ‚úÖ Lesson 2.1 Checklist

### Environment Setup
- [ ] Created complete project directory structure
- [ ] Set up Python virtual environment  
- [ ] Installed all required dependencies
- [ ] Verified PyTorch and CUDA installation

### Configuration System
- [ ] Created base configuration files
- [ ] Implemented configuration management system
- [ ] Tested configuration loading and validation
- [ ] Set up CIFAR-10 specific configuration

### Project Foundation
- [ ] Created modular package structure
- [ ] Implemented dataset loading utilities
- [ ] Set up logging and checkpoint directories
- [ ] Created verification and documentation

### Testing
- [ ] Ran environment verification script
- [ ] Downloaded and tested CIFAR-10 dataset
- [ ] Verified all imports work correctly
- [ ] Ready for multi-crop augmentation implementation

---

## üéØ Key Takeaways

1. **Modular Design**: Clean separation of concerns makes the codebase maintainable
2. **Configuration Management**: YAML configs enable easy experimentation
3. **Environment Setup**: Proper dependency management prevents version conflicts  
4. **Verification**: Always test your setup before proceeding
5. **Documentation**: Good documentation saves time in the long run

**Next**: üîπ Lesson 2.2 - Multi-Crop Data Augmentation Pipeline

Your DINO implementation foundation is now ready! In the next lesson, we'll implement the critical multi-crop augmentation strategy that makes DINO work effectively.
